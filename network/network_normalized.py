import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Lambda, Concatenate, LeakyReLU,BatchNormalization, Dropout, LayerNormalization


tf.keras.backend.set_floatx('float32')


# è¾“å…¥çš„çŠ¶æ€æ˜¯åŸå§‹çš„ï¼Œè¿›è¡Œå½’ä¸€åŒ–çŠ¶æ€ã€‚å¾—åˆ°çš„åŠ¨ä½œæ˜¯å½’ä¸€åŒ–çš„

class network:
    def __init__(self, state_dim, action_dim_continuous, action_dim_discrete,
                 action_bound, action_shift, state_low, state_high):
        """
        åˆå§‹åŒ–ç½‘ç»œ
        :param state_dim: çŠ¶æ€ç»´åº¦
        :param action_dim_continuous: è¿ç»­åŠ¨ä½œç»´åº¦
        :param action_dim_discrete: ç¦»æ•£åŠ¨ä½œç»´åº¦
        :param action_bound: è¿ç»­åŠ¨ä½œçš„èŒƒå›´
        :param action_shift: è¿ç»­åŠ¨ä½œçš„åç§»é‡
        :param state_low: çŠ¶æ€çš„ä¸‹ç•Œï¼Œåˆ—è¡¨æˆ–æ•°ç»„
        :param state_high: çŠ¶æ€çš„ä¸Šç•Œï¼Œåˆ—è¡¨æˆ–æ•°ç»„
        """
        self.state_dim = state_dim
        self.action_dim_continuous = action_dim_continuous
        # self.action_dim_discrete = action_dim_discrete
        self.action_dim_discrete = 0
        self.action_bound = action_bound
        self.action_shift = action_shift

        # çŠ¶æ€çš„ä¸Šä¸‹é™
        self.state_low = np.array(state_low,dtype=np.float32)
        self.state_high = np.array(state_high,dtype=np.float32)
        self.state_range = self.state_high - self.state_low


    def actor(self, units=(512, 256, 64, 32), tau_=0.5):
        state_input = Input(shape=(self.state_dim,), name="State_Input")

        # å½’ä¸€åŒ–çŠ¶æ€
        normalized_state = Lambda(
            lambda x: (x - self.state_low) / self.state_range,
            name="State_Normalization"
        )(state_input)

        # éšè—å±‚
        x = Dense(units[0], kernel_initializer=tf.keras.initializers.he_uniform(),name="L0")(normalized_state)
        x = LeakyReLU()(x)
        for index in range(1, len(units)):
            x = Dense(units[index], kernel_initializer=tf.keras.initializers.he_uniform(),name=f"L{index}")(x)
            x = LeakyReLU()(x)

        ## **è¿ç»­åŠ¨ä½œï¼šéšæœºç­–ç•¥**
        # è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
        # mu_output = Dense(self.action_dim_continuous,name="Cont_Output_Mean")(x)
        # sigma_output = Dense(self.action_dim_continuous, activation="softplus", name="Cont_Output_StdDev")(x)  # è¿™ä¸ªé™åˆ¶èŒƒå›´>0

        mu_output = Dense(self.action_dim_continuous,
                          kernel_initializer=tf.keras.initializers.he_uniform(),
                          name="Cont_Output_Mean")(x)
        mu_output = Lambda(lambda x: tf.tanh(x), name="Cont_Output_Mean_Tanh")(mu_output)  # âœ… é™åˆ¶ mu åœ¨ [-1,1] ,è¿™é‡Œå¿…é¡»è¿™ä¹ˆåšï¼Œä½ è¦ä¿è¯å’ŒåŠ¨ä½œä¸€è‡´

        sigma_output = Dense(self.action_dim_continuous, activation="softplus",
                             kernel_initializer=tf.keras.initializers.he_uniform(),
                             name="Cont_Output_StdDev")(x)

        # âœ… sigmaä¸è¦æ¥è¿‘äº0
        sigma_output = Lambda(lambda x: tf.clip_by_value(x, 0.1, np.inf))(sigma_output)

        # é‡æ–°å‚æ•°åŒ–æŠ€å·§ï¼ˆReparameterization Trickï¼‰ç»™å‡ºè¿ç»­åŠ¨ä½œ
        # å†™æˆå¯å¾®å½¢å¼ï¼Œæ–¹ä¾¿æ±‚å¯¼
        epsilon = Lambda(lambda x: tf.random.normal(shape=tf.shape(x)))(sigma_output)
        sampled_cont_action = Lambda(lambda x: x[0] + x[1] * x[2])([mu_output, sigma_output, epsilon])
        sampled_cont_action = Lambda(lambda x: tf.tanh(x))(sampled_cont_action)  # âœ… æœ€ç»ˆåŠ¨ä½œå½’ä¸€åŒ–ï¼Œç»è¿‡tanh

        ## **ç¦»æ•£åŠ¨ä½œï¼ˆå¦‚æœ action_dim_discrete > 0ï¼‰**
        if self.action_dim_discrete > 0:
            # é¢„æµ‹ logitsï¼ˆæœªå½’ä¸€åŒ–æ¦‚ç‡ï¼‰
            logits = Dense(self.action_dim_discrete,
                           kernel_initializer=tf.keras.initializers.he_uniform(),
                           name="Disc_Output_Logits")(x)

            # Gumbel-Softmax é‡‡æ ·ï¼šç¦»æ•£åŠ¨ä½œèŒƒå›´å±äº0-1
            def gumbel_softmax_sample(logits, tau_=0.5):
                """ Gumbel-Softmax é‡‡æ ·å‡½æ•° """
                uniform_noise = tf.random.uniform(tf.shape(logits), minval=1e-8, maxval=1)
                gumbel_noise = -tf.math.log(-tf.math.log(uniform_noise))
                return tf.nn.softmax((logits + gumbel_noise) / tau_)

            sampled_disc_action = Lambda(lambda x: gumbel_softmax_sample(x, tau_), name="Disc_Output_Gumbel")(logits)

            model = Model(inputs=state_input,
                          outputs=[mu_output, sigma_output, sampled_cont_action, sampled_disc_action,logits])
        else:
            model = Model(inputs=state_input, outputs=[mu_output, sigma_output, sampled_cont_action])

        return model

        ## **ğŸš€ åŒ Q Critic ç½‘ç»œ**

    # è¾“å…¥åŸå§‹åŠ¨ä½œå’Œactorå¾—å‡ºçš„åŠ¨ä½œ
    def critic(self, units=(128, 128, 32)):
        """ **åˆ›å»ºä¸¤ä¸ªç‹¬ç«‹çš„ Q ç½‘ç»œï¼šQ1 å’Œ Q2** """

        def build_q_network():
            state_input = Input(shape=(self.state_dim,), name="State_Input")
            cont_action_input = Input(shape=(self.action_dim_continuous,), name="Cont_Action_Input")

            # **çŠ¶æ€å½’ä¸€åŒ–**
            normalized_state = Lambda(
                lambda x: (x - self.state_low) / self.state_range,
                name="State_Normalization"
            )(state_input)

            if self.action_dim_discrete > 0:
                disc_action_input = Input(shape=(self.action_dim_discrete,), name="Disc_Action_Input")
                concat = Concatenate(axis=-1, name="Concat_Input")(
                    [normalized_state, cont_action_input, disc_action_input])
                inputs = [state_input, cont_action_input, disc_action_input]
            else:
                concat = Concatenate(axis=-1, name="Concat_Input")([normalized_state, cont_action_input])
                inputs = [state_input, cont_action_input]

            # **éšè—å±‚**
            x = Dense(units[0], kernel_initializer=tf.keras.initializers.he_uniform(), name="L0")(concat)
            x = LeakyReLU()(x)
            x = LayerNormalization(name="LayerNorm0")(x)  # ğŸš€ å¢åŠ  Layer Normalization
            for index in range(1, len(units)):
                x = Dense(units[index], kernel_initializer=tf.keras.initializers.he_uniform(), name=f"L{index}")(x)
                x = LeakyReLU()(x)
                x = Dropout(0.1, name=f"Dropout{index}")(x)  # ğŸš€ æ·»åŠ  Dropout

            # **å•ç‹¬çš„ Q å€¼è¾“å‡º**
            q_value_output = Dense(1, activation="linear", name="Q_Value_Output")(x)


            return Model(inputs=inputs, outputs=q_value_output)

        # **åˆ›å»ºä¸¤ä¸ª Q ç½‘ç»œ**
        q1_model = build_q_network()
        q2_model = build_q_network()

        return q1_model, q2_model

        ## **ğŸš€ çº¦æŸ Critic Q_C**

    def constraint_critic(self, units=(128, 128, 32)):
        """ **åˆ›å»ºå•ç‹¬çš„çº¦æŸ Critic Q_C** """
        state_input = Input(shape=(self.state_dim,), name="State_Input")
        cont_action_input = Input(shape=(self.action_dim_continuous,), name="Cont_Action_Input")

        # **çŠ¶æ€å½’ä¸€åŒ–**
        normalized_state = Lambda(
            lambda x: (x - self.state_low) / self.state_range,
            name="State_Normalization"
        )(state_input)

        if self.action_dim_discrete > 0:
            disc_action_input = Input(shape=(self.action_dim_discrete,), name="Disc_Action_Input")
            concat = Concatenate(axis=-1, name="Concat_Input")([normalized_state, cont_action_input, disc_action_input])
            inputs = [state_input, cont_action_input, disc_action_input]
        else:
            concat = Concatenate(axis=-1, name="Concat_Input")([normalized_state, cont_action_input])
            inputs = [state_input, cont_action_input]

        # âœ… é€å±‚æ„å»º Q_c ç½‘ç»œï¼ˆåŠ  LayerNorm & Dropout é˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
        x = Dense(units[0], kernel_initializer=tf.keras.initializers.he_uniform(), name="L0")(concat)
        x = LeakyReLU()(x)
        x = LayerNormalization(name="LayerNorm0")(x)  # ğŸš€ å¢åŠ  Layer Normalization
        for index in range(1, len(units)):
            x = Dense(units[index], kernel_initializer=tf.keras.initializers.he_uniform(), name=f"L{index}")(x)
            x = LeakyReLU()(x)
            x = Dropout(0.1, name=f"Dropout{index}")(x)  # ğŸš€ æ·»åŠ  Dropout

        # **çº¦æŸ Q å€¼è¾“å‡º**
        qc_value_output = Dense(1, activation="linear", name="Q_Constraint_Output")(x)

        return Model(inputs=inputs, outputs=qc_value_output)

    def update_target_weights(self, model, target_model, tau=np.float32(0.01)):
        weights = model.get_weights()
        target_weights = target_model.get_weights()
        for i in range(len(target_weights)):
            target_weights[i] = weights[i] * tau + target_weights[i] * (1 - tau)
        target_model.set_weights(target_weights)



if __name__=="__main__":
    print("Imports worked!")